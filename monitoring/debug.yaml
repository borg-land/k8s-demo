---
# Source: monitoring/templates/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    istio-injection: enabled
---
# Source: monitoring/templates/limitrange.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: monitoring
spec:
  limits:
  - default:
      cpu: 0.3
      memory: 512Mi
    defaultRequest:
      cpu: 0.1
      memory: 256Mi
    type: Container
---
# Source: monitoring/charts/mimir-distributed/templates/podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: monitoring-mimir
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
  annotations:
    "seccomp.security.alpha.kubernetes.io/allowedProfileNames": runtime/default
spec:
  privileged: false
  allowPrivilegeEscalation: false
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'persistentVolumeClaim'
    - 'secret'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
    - min: 1
      max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
    - min: 1
      max: 65535
  readOnlyRootFilesystem: true
  requiredDropCapabilities:
    - ALL
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: monitoring-mimir-ingester
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: ingester
  maxUnavailable: 1
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: monitoring-mimir-store-gateway
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: store-gateway
  maxUnavailable: 1
---
# Source: monitoring/charts/grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    iam.gke.io/gcp-service-account: grafana@coen-mahamed-ali.iam.gserviceaccount.com
  name: grafana
  namespace: monitoring
---
# Source: monitoring/charts/mimir-distributed/charts/rollout_operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: monitoring-rollout-operator
  labels:
    helm.sh/chart: rollout-operator-0.1.2
    app.kubernetes.io/name: rollout-operator
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "v0.1.1"
    app.kubernetes.io/managed-by: Helm
---
# Source: monitoring/charts/mimir-distributed/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mimir
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    iam.gke.io/gcp-service-account: thanos@coen-mahamed-ali.iam.gserviceaccount.com
  namespace: "monitoring"
---
# Source: monitoring/charts/grafana/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: monitoring-grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  admin-user: "YWRtaW4="
  admin-password: "SExhOTlYQlU0dUd3UExSNkMwQ3dXcmZlUGRWbDFoYXByNjJVSTRtSA=="
  ldap-toml: ""
---
# Source: monitoring/charts/grafana/templates/configmap-dashboard-provider.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
  name: monitoring-grafana-config-dashboards
  namespace: monitoring
data:
  provider.yaml: |-
    apiVersion: 1
    providers:
    - name: 'sidecarProvider'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      allowUiUpdates: false
      updateIntervalSeconds: 30
      options:
        foldersFromFilesStructure: false
        path: /tmp/dashboards
---
# Source: monitoring/charts/grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
data:
  plugins: grafana-worldmap-panel,grafana-piechart-panel,grafana-polystat-panel,grafana-x-ray-datasource,grafana-athena-datasource
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [auth.github]
    allow_sign_up = true
    api_url = https://api.github.com/user
    auth_url = https://github.com/login/oauth/authorize
    enabled = false
    scopes = user:email,read:org
    token_url = https://github.com/login/oauth/access_token
    [feature_toggles]
    publicDashboards = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning
    [server]
    domain = ''
    root_url = https://grafana.knative.dev

  datasources.yaml: |
    apiVersion: 1
    datasources:
    - access: proxy
      isDefault: true
      name: Prometheus
      timeout: 120
      type: prometheus
      url: http://frontend.monitoring:9090
---
# Source: monitoring/charts/mimir-distributed/templates/mimir-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-mimir-config
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
data:
  mimir.yaml: |
    
    activity_tracker:
      filepath: /active-query-tracker/activity.log
    alertmanager:
      data_dir: /data
      enable_api: true
      external_url: /alertmanager
    blocks_storage:
      backend: gcs
      bucket_store:
        chunks_cache:
          backend: memcached
          memcached:
            addresses: 10.104.128.67:11211
            max_item_size: 1048576
            timeout: 450ms
        index_cache:
          backend: memcached
          memcached:
            addresses: 10.104.128.67:11211
            max_item_size: 15728640
        metadata_cache:
          backend: memcached
          memcached:
            addresses: 10.104.128.67:11211
            max_item_size: 1048576
        sync_dir: /data/tsdb-sync
      gcs:
        bucket_name: coen-thanos
      tsdb:
        dir: /data/tsdb
    compactor:
      data_dir: /data
    frontend:
      align_queries_with_step: true
      log_queries_longer_than: 10s
      parallelize_shardable_queries: true
      scheduler_address: monitoring-mimir-query-scheduler-headless.monitoring.svc:9095
    frontend_worker:
      scheduler_address: monitoring-mimir-query-scheduler-headless.monitoring.svc:9095
    ingester:
      ring:
        final_sleep: 0s
        num_tokens: 512
        tokens_file_path: /data/tokens
        unregister_on_shutdown: false
        zone_awareness_enabled: true
    ingester_client:
      grpc_client_config:
        max_recv_msg_size: 104857600
        max_send_msg_size: 104857600
    limits:
      max_query_parallelism: 224
    memberlist:
      abort_if_cluster_join_fails: false
      compression_enabled: false
      join_members:
      - dns+monitoring-mimir-gossip-ring.monitoring.svc.cluster.local:7946
    querier:
      max_concurrent: 16
    query_scheduler:
      max_outstanding_requests_per_tenant: 1600
    ruler:
      alertmanager_url: dnssrvnoa+http://_http-metrics._tcp.monitoring-mimir-alertmanager-headless.monitoring.svc.cluster.local/alertmanager
      enable_api: true
      rule_path: /data
    runtime_config:
      file: /var/mimir/runtime.yaml
    server:
      grpc_server_max_concurrent_streams: 1000
      grpc_server_max_connection_age: 2m
      grpc_server_max_connection_age_grace: 5m
      grpc_server_max_connection_idle: 1m
    store_gateway:
      sharding_ring:
        kvstore:
          prefix: multi-zone/
        tokens_file_path: /data/tokens
        zone_awareness_enabled: true
---
# Source: monitoring/charts/mimir-distributed/templates/nginx/nginx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-mimir-nginx
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: nginx
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
data:
  nginx.conf: |
    worker_processes  5;  ## Default: 1
    error_log  /dev/stderr;
    pid        /tmp/nginx.pid;
    worker_rlimit_nofile 8192;
    
    events {
      worker_connections  4096;  ## Default: 1024
    }
    
    http {
      client_body_temp_path /tmp/client_temp;
      proxy_temp_path       /tmp/proxy_temp_path;
      fastcgi_temp_path     /tmp/fastcgi_temp;
      uwsgi_temp_path       /tmp/uwsgi_temp;
      scgi_temp_path        /tmp/scgi_temp;
    
      default_type application/octet-stream;
      log_format   main '$remote_addr - $remote_user [$time_local]  $status '
            '"$request" $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';
      access_log   /dev/stderr  main;
    
      sendfile     on;
      tcp_nopush   on;
      resolver kube-dns.kube-system.svc.cluster.local;
    
      # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
      map $http_x_scope_orgid $ensured_x_scope_orgid {
        default $http_x_scope_orgid;
        "" "anonymous";
      }
    
      server {
        listen 8080;
    
        location = / {
          return 200 'OK';
          auth_basic off;
        }
    
        proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;
    
        # Distributor endpoints
        location /distributor {
          proxy_pass      http://monitoring-mimir-distributor-headless.monitoring.svc.cluster.local:8080$request_uri;
        }
        location = /api/v1/push {
          proxy_pass      http://monitoring-mimir-distributor-headless.monitoring.svc.cluster.local:8080$request_uri;
        }
    
        # Alertmanager endpoints
        location /alertmanager {
          proxy_pass      http://monitoring-mimir-alertmanager-headless.monitoring.svc.cluster.local:8080$request_uri;
        }
        location = /multitenant_alertmanager/status {
          proxy_pass      http://monitoring-mimir-alertmanager-headless.monitoring.svc.cluster.local:8080$request_uri;
        }
        location = /api/v1/alerts {
          proxy_pass      http://monitoring-mimir-alertmanager-headless.monitoring.svc.cluster.local:8080$request_uri;
        }
    
        # Ruler endpoints
        location /prometheus/config/v1/rules {
          proxy_pass      http://monitoring-mimir-ruler.monitoring.svc.cluster.local:8080$request_uri;
        }
        location /prometheus/api/v1/rules {
          proxy_pass      http://monitoring-mimir-ruler.monitoring.svc.cluster.local:8080$request_uri;
        }
    
        location /prometheus/api/v1/alerts {
          proxy_pass      http://monitoring-mimir-ruler.monitoring.svc.cluster.local:8080$request_uri;
        }
        location = /ruler/ring {
          proxy_pass      http://monitoring-mimir-ruler.monitoring.svc.cluster.local:8080$request_uri;
        }
    
        # Rest of /prometheus goes to the query frontend
        location /prometheus {
          proxy_pass      http://monitoring-mimir-query-frontend.monitoring.svc.cluster.local:8080$request_uri;
        }
    
        # Buildinfo endpoint can go to any component
        location = /api/v1/status/buildinfo {
          proxy_pass      http://monitoring-mimir-query-frontend.monitoring.svc.cluster.local:8080$request_uri;
        }
    
        # Compactor endpoint for uploading blocks
        location /api/v1/upload/block/ {
          proxy_pass      http://monitoring-mimir-compactor.monitoring.svc.cluster.local:8080$request_uri;
        }
      }
    }
---
# Source: monitoring/charts/mimir-distributed/templates/runtime-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-mimir-runtime
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
data:
  runtime.yaml: |
    
    {}
---
# Source: monitoring/charts/grafana/templates/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: monitoring-grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
  finalizers:
    - kubernetes.io/pvc-protection
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "50Gi"
---
# Source: monitoring/charts/grafana/templates/clusterrole.yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
  name: monitoring-grafana-clusterrole
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["configmaps", "secrets"]
  verbs: ["get", "watch", "list"]
---
# Source: monitoring/charts/grafana/templates/clusterrolebinding.yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: monitoring-grafana-clusterrolebinding
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
subjects:
  - kind: ServiceAccount
    name: grafana
    namespace: monitoring
roleRef:
  kind: ClusterRole
  name: monitoring-grafana-clusterrole
  apiGroup: rbac.authorization.k8s.io
---
# Source: monitoring/charts/grafana/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: monitoring-grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [monitoring-grafana]
---
# Source: monitoring/charts/mimir-distributed/charts/rollout_operator/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: monitoring-rollout-operator
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - list
  - get
  - watch
  - delete
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs:
  - list
  - get
  - watch
- apiGroups:
  - apps
  resources:
  - statefulsets/status
  verbs:
  - update
---
# Source: monitoring/charts/mimir-distributed/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: monitoring-mimir
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
rules:
- apiGroups:      ['extensions']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [monitoring-mimir]
---
# Source: monitoring/charts/grafana/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: monitoring-grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: monitoring-grafana
subjects:
- kind: ServiceAccount
  name: grafana
  namespace: monitoring
---
# Source: monitoring/charts/mimir-distributed/charts/rollout_operator/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: monitoring-rollout-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: monitoring-rollout-operator
subjects:
- kind: ServiceAccount
  name: monitoring-rollout-operator
---
# Source: monitoring/charts/mimir-distributed/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: monitoring-mimir
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: monitoring-mimir
subjects:
- kind: ServiceAccount
  name: mimir
---
# Source: monitoring/charts/grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: service
      port: 80
      protocol: TCP
      targetPort: 3000
  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
---
# Source: monitoring/charts/mimir-distributed/templates/alertmanager/alertmanager-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-alertmanager-headless
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
    - port: 9094
      protocol: TCP
      name: cluster
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: alertmanager
---
# Source: monitoring/charts/mimir-distributed/templates/alertmanager/alertmanager-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-alertmanager
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: alertmanager
---
# Source: monitoring/charts/mimir-distributed/templates/compactor/compactor-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-compactor
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: compactor
---
# Source: monitoring/charts/mimir-distributed/templates/distributor/distributor-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-distributor-headless
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: distributor
---
# Source: monitoring/charts/mimir-distributed/templates/distributor/distributor-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-distributor
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: distributor
---
# Source: monitoring/charts/mimir-distributed/templates/gossip-ring/gossip-ring-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-gossip-ring
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: gossip-ring
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  namespace: "monitoring"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: gossip-ring
      port: 7946
      protocol: TCP
      targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/part-of: memberlist
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-ingester-headless
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-ingester-zone-a
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "ingester-zone-a"
    rollout-group: ingester
    zone: zone-a
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    rollout-group: ingester
    zone: zone-a
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-ingester-zone-b
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "ingester-zone-b"
    rollout-group: ingester
    zone: zone-b
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    rollout-group: ingester
    zone: zone-b
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-ingester-zone-c
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "ingester-zone-c"
    rollout-group: ingester
    zone: zone-c
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    rollout-group: ingester
    zone: zone-c
---
# Source: monitoring/charts/mimir-distributed/templates/nginx/nginx-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-nginx
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: nginx
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - name: http-metric
      port: 80
      targetPort: http-metric
      protocol: TCP
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: nginx
---
# Source: monitoring/charts/mimir-distributed/templates/overrides-exporter/overrides-exporter-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-overrides-exporter
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: overrides-exporter
---
# Source: monitoring/charts/mimir-distributed/templates/querier/querier-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-querier
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: querier
---
# Source: monitoring/charts/mimir-distributed/templates/query-frontend/query-frontend-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-query-frontend-headless
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-frontend
---
# Source: monitoring/charts/mimir-distributed/templates/query-frontend/query-frontend-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-query-frontend
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-frontend
---
# Source: monitoring/charts/mimir-distributed/templates/query-scheduler/query-scheduler-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-query-scheduler-headless
  namespace: "monitoring"
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-scheduler
---
# Source: monitoring/charts/mimir-distributed/templates/query-scheduler/query-scheduler-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-query-scheduler
  namespace: "monitoring"
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-scheduler
---
# Source: monitoring/charts/mimir-distributed/templates/ruler/ruler-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-ruler
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ruler
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ruler
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-store-gateway-headless
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    prometheus.io/service-monitor: "false"
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-store-gateway-zone-a
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "store-gateway-zone-a"
    rollout-group: store-gateway
    zone: zone-a
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    rollout-group: store-gateway
    zone: zone-a
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-store-gateway-zone-b
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "store-gateway-zone-b"
    rollout-group: store-gateway
    zone: zone-b
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    rollout-group: store-gateway
    zone: zone-b
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: monitoring-mimir-store-gateway-zone-c
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "store-gateway-zone-c"
    rollout-group: store-gateway
    zone: zone-c
  annotations:
    {}
  namespace: "monitoring"
spec:
  type: ClusterIP
  ports:
    - port: 8080
      protocol: TCP
      name: http-metrics
      targetPort: http-metrics
    - port: 9095
      protocol: TCP
      name: grpc
      targetPort: grpc
  selector:
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    rollout-group: store-gateway
    zone: zone-c
---
# Source: monitoring/templates/proxy.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend
  namespace: monitoring
spec:
  selector:
    app: frontend
  ports:
  - name: web
    port: 9090
---
# Source: monitoring/charts/grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-grafana
  namespace: monitoring
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: monitoring
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: monitoring
      annotations:
        checksum/config: b0fdec63f26761f260a1b8e797c06d829288900dc9e2ff6f40dd752994e50e18
        checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/sc-dashboard-provider-config: 81e5e3b8c8db0ec15bb3e86297e4f344e882d8f580b78d930e4de23dc054801b
        checksum/secret: 1672c92c6fd3d2b5fce15786b4b4c00e0058884f113db197ea28944e9418ef0a
    spec:      
      serviceAccountName: grafana
      automountServiceAccountToken: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsUser: 472
      initContainers:
        - name: init-chown-data
          image: "busybox:1.31.1"
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsNonRoot: false
            runAsUser: 0
          command: ["chown", "-R", "472:472", "/var/lib/grafana"]
          volumeMounts:
            - name: storage
              mountPath: "/var/lib/grafana"
      enableServiceLinks: true
      containers:
        - name: grafana-sc-dashboard
          image: "quay.io/kiwigrid/k8s-sidecar:1.19.2"
          imagePullPolicy: IfNotPresent
          env:
            - name: METHOD
              value: WATCH
            - name: LABEL
              value: "grafana_dashboard"
            - name: FOLDER
              value: "/tmp/dashboards"
            - name: RESOURCE
              value: "both"
            - name: NAMESPACE
              value: "ALL"
          volumeMounts:
            - name: sc-dashboard-volume
              mountPath: "/tmp/dashboards"
        - name: grafana
          image: "grafana/grafana:9.2.1"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: "datasources.yaml"
            - name: sc-dashboard-volume
              mountPath: "/tmp/dashboards"
      
            - name: sc-dashboard-provider
              mountPath: "/etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml"
              subPath: provider.yaml
            - name: aws-iam-token
              mountPath: /var/run/secrets/aws-iam-token/serviceaccount
              readOnly: true
              subPath: 
          ports:
            - name: grafana
              containerPort: 3000
              protocol: TCP
          env:
            - name: GF_SECURITY_ADMIN_USER
              valueFrom:
                secretKeyRef:
                  name: monitoring-grafana
                  key: admin-user
            - name: GF_SECURITY_ADMIN_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: monitoring-grafana
                  key: admin-password
            - name: GF_INSTALL_PLUGINS
              valueFrom:
                configMapKeyRef:
                  name: monitoring-grafana
                  key: plugins
            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
            - name: "AWS_REGION"
              value: "eu-central-1"
            - name: "AWS_ROLE_ARN"
              value: "arn:aws:iam::691556712356:role/gke-grafana"
            - name: "AWS_ROLE_SESSION_NAME"
              value: "grafana"
            - name: "AWS_WEB_IDENTITY_TOKEN_FILE"
              value: "/var/run/secrets/aws-iam-token/serviceaccount/token"
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            limits:
              cpu: 4000m
              memory: 2048Mi
            requests:
              cpu: 500m
              memory: 1024Mi
      volumes:
        - name: config
          configMap:
            name: monitoring-grafana
        - name: storage
          persistentVolumeClaim:
            claimName: monitoring-grafana
        - name: sc-dashboard-volume
          emptyDir: {}
        - name: sc-dashboard-provider
          configMap:
            name: monitoring-grafana-config-dashboards
        - name: aws-iam-token
          projected:
            defaultMode: 420
            sources:
            - serviceAccountToken:
                audience: sts.amazonaws.com
                expirationSeconds: 86400
                path: token
---
# Source: monitoring/charts/mimir-distributed/charts/rollout_operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-rollout-operator
  labels:
    helm.sh/chart: rollout-operator-0.1.2
    app.kubernetes.io/name: rollout-operator
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "v0.1.1"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  minReadySeconds: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: rollout-operator
      app.kubernetes.io/instance: monitoring
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
  template:
    metadata:
      labels:
        app.kubernetes.io/name: rollout-operator
        app.kubernetes.io/instance: monitoring
    spec:
      serviceAccountName: monitoring-rollout-operator
      securityContext:
        {}
      containers:
        - name: rollout-operator
          securityContext:
            {}
          image: "grafana/rollout-operator:v0.1.1"
          imagePullPolicy: IfNotPresent
          args:
          - -kubernetes.namespace=monitoring
          ports:
            - name: http-metrics
              containerPort: 8001
              protocol: TCP
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 5
            timeoutSeconds: 1
          resources:
            limits:
              cpu: "1"
              memory: 200Mi
            requests:
              cpu: 100m
              memory: 100Mi
---
# Source: monitoring/charts/mimir-distributed/templates/distributor/distributor-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-mimir-distributor
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: distributor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: distributor
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: distributor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      containers:
        - name: distributor
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=distributor"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            limits:
              cpu: 4
              memory: 5.7Gi
            requests:
              cpu: 2
              memory: 4Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: distributor
      tolerations:
        []
      terminationGracePeriodSeconds: 60
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: monitoring/charts/mimir-distributed/templates/nginx/nginx-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-mimir-nginx
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: nginx
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: nginx
  template:
    metadata:
      annotations:
        checksum/config: a9b42000f969c9d70249c8f384684cf4b7ac6fcb5ebdf2fa51a846d33617bd08
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: nginx
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      terminationGracePeriodSeconds: 30
      containers:
        - name: nginx
          image: docker.io/nginxinc/nginx-unprivileged:1.22-alpine
          imagePullPolicy: IfNotPresent
          ports:
            - name: http-metric
              containerPort: 8080
              protocol: TCP
          env:
          envFrom:
          readinessProbe:
            httpGet:
              path: /
              port: http-metric
            initialDelaySeconds: 15
            timeoutSeconds: 1
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          volumeMounts:
            - name: config
              mountPath: /etc/nginx
            - name: tmp
              mountPath: /tmp
            - name: docker-entrypoint-d-override
              mountPath: /docker-entrypoint.d
          resources:
            limits:
              cpu: 2
              memory: 731Mi
            requests:
              cpu: 1
              memory: 512Mi
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: nginx
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-nginx
        - name: tmp
          emptyDir: {}
        - name: docker-entrypoint-d-override
          emptyDir: {}
---
# Source: monitoring/charts/mimir-distributed/templates/overrides-exporter/overrides-exporter-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    {}
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: overrides-exporter
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  name: monitoring-mimir-overrides-exporter
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: overrides-exporter
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: overrides-exporter
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      containers:
        - name: overrides-exporter
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=overrides-exporter"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            limits:
              cpu: 500m
              memory: 128Mi
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        
      tolerations:
        []
      terminationGracePeriodSeconds: 60
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: monitoring/charts/mimir-distributed/templates/querier/querier-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-mimir-querier
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: querier
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: querier
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: querier
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      containers:
        - name: querier
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=querier"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            limits:
              cpu: 4
              memory: 5.6Gi
            requests:
              cpu: 2
              memory: 4Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: querier
      tolerations:
        []
      terminationGracePeriodSeconds: 180
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: monitoring/charts/mimir-distributed/templates/query-frontend/query-frontend-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-mimir-query-frontend
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-frontend
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: query-frontend
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: query-frontend
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      containers:
        - name: query-frontend
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=query-frontend"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: runtime-config
              mountPath: /var/mimir
            - name: config
              mountPath: /etc/mimir
            - name: storage
              mountPath: /data
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            limits:
              cpu: 4
              memory: 2.8Gi
            requests:
              cpu: 2
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: query-frontend
      tolerations:
        []
      terminationGracePeriodSeconds: 180
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: monitoring/charts/mimir-distributed/templates/query-scheduler/query-scheduler-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-mimir-query-scheduler
  namespace: "monitoring"
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: query-scheduler
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: query-scheduler
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: query-scheduler
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      containers:
        - name: query-scheduler
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=query-scheduler"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-server.grpc.keepalive.max-connection-age=2562047h" # 100000 days, effectively infinity
            - "-server.grpc.keepalive.max-connection-age-grace=2562047h" # 100000 days, effectively infinity
          volumeMounts:
            - name: runtime-config
              mountPath: /var/mimir
            - name: config
              mountPath: /etc/mimir
            - name: storage
              mountPath: /data
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: query-scheduler
      tolerations:
        []
      terminationGracePeriodSeconds: 180
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: monitoring/charts/mimir-distributed/templates/ruler/ruler-dep.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: monitoring-mimir-ruler
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ruler
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: ruler
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ruler
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      containers:
        - name: ruler
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=ruler"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
              subPath: 
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            limits:
              cpu: 2
              memory: 2.8Gi
            requests:
              cpu: 1
              memory: 2Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: ruler
      tolerations:
        []
      terminationGracePeriodSeconds: 180
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: storage
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
---
# Source: monitoring/templates/proxy.yaml
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https:#www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: monitoring
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      automountServiceAccountToken: true
      nodeSelector:
        kubernetes.io/os: linux
        kubernetes.io/arch: amd64
      containers:
      - name: frontend
        image: "gke.gcr.io/prometheus-engine/frontend:v0.4.3-gke.0"
        args:
        - "--web.listen-address=:9090"
        - "--query.project-id=knative-tests"
        ports:
        - name: web
          containerPort: 9090
        readinessProbe:
          httpGet:
            path: /-/ready
            port: web
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: web
      serviceAccountName: grafana
---
# Source: monitoring/charts/mimir-distributed/templates/alertmanager/alertmanager-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-alertmanager
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: alertmanager
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: alertmanager
  updateStrategy:
    type: RollingUpdate
  serviceName: monitoring-mimir-alertmanager
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "1Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: alertmanager
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: alertmanager
      tolerations:
        []
      terminationGracePeriodSeconds: 60
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: tmp
          emptyDir: {}
        - name: active-queries
          emptyDir: {}
      containers:
        - name: alertmanager
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=alertmanager"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: tmp
              mountPath: /tmp
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 45
          resources:
            limits:
              cpu: 2
              memory: 1.4Gi
            requests:
              cpu: 1
              memory: 1Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/compactor/compactor-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-compactor
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: compactor
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    {}
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: compactor
  updateStrategy:
    type: RollingUpdate
  serviceName: monitoring-mimir-compactor
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "20Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: compactor
        app.kubernetes.io/part-of: memberlist
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        {}
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: compactor
      tolerations:
        []
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: compactor
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=compactor"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            limits:
              cpu: 2
              memory: 2.1Gi
            requests:
              cpu: 1
              memory: 1.5Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-ingester-zone-a
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "ingester-zone-a"
    rollout-group: ingester
    zone: zone-a
  annotations:
    rollout-max-unavailable: "25"
  namespace: "monitoring"
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: ingester
      rollout-group: ingester
      zone: zone-a
  updateStrategy:
    type: OnDelete
  serviceName: monitoring-mimir-ingester-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "50Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
        name: "ingester-zone-a"
        rollout-group: ingester
        zone: zone-a
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: rollout-group
                operator: In
                values:
                - ingester
              - key: app.kubernetes.io/component
                operator: NotIn
                values:
                - ingester-zone-a
            topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: ingester
      tolerations:
        []
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: ingester
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=ingester"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-ingester.ring.instance-availability-zone=zone-a"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            limits:
              cpu: 4
              memory: 12Gi
            requests:
              cpu: 3.5
              memory: 8Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-ingester-zone-b
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "ingester-zone-b"
    rollout-group: ingester
    zone: zone-b
  annotations:
    rollout-max-unavailable: "25"
  namespace: "monitoring"
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: ingester
      rollout-group: ingester
      zone: zone-b
  updateStrategy:
    type: OnDelete
  serviceName: monitoring-mimir-ingester-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "50Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
        name: "ingester-zone-b"
        rollout-group: ingester
        zone: zone-b
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: rollout-group
                operator: In
                values:
                - ingester
              - key: app.kubernetes.io/component
                operator: NotIn
                values:
                - ingester-zone-b
            topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: ingester
      tolerations:
        []
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: ingester
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=ingester"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-ingester.ring.instance-availability-zone=zone-b"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            limits:
              cpu: 4
              memory: 12Gi
            requests:
              cpu: 3.5
              memory: 8Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/ingester/ingester-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-ingester-zone-c
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: ingester
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "ingester-zone-c"
    rollout-group: ingester
    zone: zone-c
  annotations:
    rollout-max-unavailable: "25"
  namespace: "monitoring"
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: ingester
      rollout-group: ingester
      zone: zone-c
  updateStrategy:
    type: OnDelete
  serviceName: monitoring-mimir-ingester-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "50Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: ingester
        app.kubernetes.io/part-of: memberlist
        name: "ingester-zone-c"
        rollout-group: ingester
        zone: zone-c
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: rollout-group
                operator: In
                values:
                - ingester
              - key: app.kubernetes.io/component
                operator: NotIn
                values:
                - ingester-zone-c
            topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: ingester
      tolerations:
        []
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: ingester
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=ingester"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-ingester.ring.instance-availability-zone=zone-c"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            limits:
              cpu: 4
              memory: 12Gi
            requests:
              cpu: 3.5
              memory: 8Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-store-gateway-zone-a
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "store-gateway-zone-a"
    rollout-group: store-gateway
    zone: zone-a
  annotations:
    rollout-max-unavailable: "10"
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: store-gateway
      rollout-group: store-gateway
      zone: zone-a
  updateStrategy:
    type: OnDelete
  serviceName: monitoring-mimir-store-gateway-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "10Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: store-gateway
        app.kubernetes.io/part-of: memberlist
        name: "store-gateway-zone-a"
        rollout-group: store-gateway
        zone: zone-a
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: rollout-group
                operator: In
                values:
                - store-gateway
              - key: app.kubernetes.io/component
                operator: NotIn
                values:
                - store-gateway-zone-a
            topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: store-gateway
      tolerations:
        []
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: store-gateway
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=store-gateway"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-store-gateway.sharding-ring.instance-availability-zone=zone-a"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            limits:
              cpu: 2
              memory: 2.1Gi
            requests:
              cpu: 1
              memory: 1.5Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-store-gateway-zone-b
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "store-gateway-zone-b"
    rollout-group: store-gateway
    zone: zone-b
  annotations:
    rollout-max-unavailable: "10"
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: store-gateway
      rollout-group: store-gateway
      zone: zone-b
  updateStrategy:
    type: OnDelete
  serviceName: monitoring-mimir-store-gateway-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "10Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: store-gateway
        app.kubernetes.io/part-of: memberlist
        name: "store-gateway-zone-b"
        rollout-group: store-gateway
        zone: zone-b
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: rollout-group
                operator: In
                values:
                - store-gateway
              - key: app.kubernetes.io/component
                operator: NotIn
                values:
                - store-gateway-zone-b
            topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: store-gateway
      tolerations:
        []
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: store-gateway
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=store-gateway"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-store-gateway.sharding-ring.instance-availability-zone=zone-b"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            limits:
              cpu: 2
              memory: 2.1Gi
            requests:
              cpu: 1
              memory: 1.5Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/store-gateway/store-gateway-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: monitoring-mimir-store-gateway-zone-c
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: store-gateway
    app.kubernetes.io/part-of: memberlist
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
    name: "store-gateway-zone-c"
    rollout-group: store-gateway
    zone: zone-c
  annotations:
    rollout-max-unavailable: "10"
  namespace: "monitoring"
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: mimir
      app.kubernetes.io/instance: monitoring
      app.kubernetes.io/component: store-gateway
      rollout-group: store-gateway
      zone: zone-c
  updateStrategy:
    type: OnDelete
  serviceName: monitoring-mimir-store-gateway-headless
  volumeClaimTemplates:
    - metadata:
        name: storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: "10Gi"
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: store-gateway
        app.kubernetes.io/part-of: memberlist
        name: "store-gateway-zone-c"
        rollout-group: store-gateway
        zone: zone-c
      annotations:
        checksum/config: 1d1bb4faec71e9dd3490d65f8b17975bf0505335fbead66d53d01033ac3960c3
      namespace: "monitoring"
    spec:
      serviceAccountName: mimir
      securityContext:
        fsGroup: 10001
        runAsGroup: 10001
        runAsNonRoot: true
        runAsUser: 10001
        seccompProfile:
          type: RuntimeDefault
      initContainers:
        []
      nodeSelector:
        {}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: rollout-group
                operator: In
                values:
                - store-gateway
              - key: app.kubernetes.io/component
                operator: NotIn
                values:
                - store-gateway-zone-c
            topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: mimir
              app.kubernetes.io/instance: monitoring
              app.kubernetes.io/component: store-gateway
      tolerations:
        []
      terminationGracePeriodSeconds: 240
      volumes:
        - name: config
          configMap:
            name: monitoring-mimir-config
            items:
              - key: "mimir.yaml"
                path: "mimir.yaml"
        - name: runtime-config
          configMap:
            name: monitoring-mimir-runtime
        - name: active-queries
          emptyDir: {}
      containers:
        - name: store-gateway
          image: "grafana/mimir:r209-bc07330"
          imagePullPolicy: IfNotPresent
          args:
            - "-target=store-gateway"
            - "-config.expand-env=true"
            - "-config.file=/etc/mimir/mimir.yaml"
            - "-store-gateway.sharding-ring.instance-availability-zone=zone-c"
          volumeMounts:
            - name: config
              mountPath: /etc/mimir
            - name: runtime-config
              mountPath: /var/mimir
            - name: storage
              mountPath: "/data"
            - name: active-queries
              mountPath: /active-query-tracker
          ports:
            - name: http-metrics
              containerPort: 8080
              protocol: TCP
            - name: grpc
              containerPort: 9095
              protocol: TCP
            - name: memberlist
              containerPort: 7946
              protocol: TCP
          livenessProbe:
            null
          readinessProbe:
            httpGet:
              path: /ready
              port: http-metrics
            initialDelaySeconds: 60
          resources:
            limits:
              cpu: 2
              memory: 2.1Gi
            requests:
              cpu: 1
              memory: 1.5Gi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
          env:
          envFrom:
---
# Source: monitoring/charts/mimir-distributed/templates/minio/create-bucket-job.yaml
# Minio provides post-install hook to create bucket
# however the hook won't be executed if helm install is run
# with --wait flag. Hence this job is a workaround for that.
# See https://github.com/grafana/mimir/issues/2464
---
# Source: monitoring/templates/istio.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  labels:
    app: grafana
  name: grafana
  namespace: monitoring
spec:
  hosts:
    - grafana.m.rack.gold
  gateways:
  - istio-system/rack-gold
  http:
  - route:
    - destination:
        host: monitoring-grafana
        port:
          number: 80
# ---
# apiVersion: networking.istio.io/v1alpha3
# kind: VirtualService
# metadata:
#   labels:
#     app: prometheus
#   name: prometheus
#   namespace: monitoring
# spec:
#   hosts:
#     - prometheus.upo.one
#   gateways:
#   - istio-system/rack-gold
#   http:
#   - route:
#     - destination:
#         host: monitoring-kube-prometheus-prometheus
#         port:
#           number: 9090
---
# Source: monitoring/charts/grafana/templates/tests/test-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
  name: monitoring-grafana-test
  namespace: monitoring
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
---
# Source: monitoring/charts/grafana/templates/tests/test-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: monitoring-grafana-test
  namespace: monitoring
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
data:
  run.sh: |-
    @test "Test Health" {
      url="http://monitoring-grafana/api/health"

      code=$(wget --server-response --spider --timeout 90 --tries 10 ${url} 2>&1 | awk '/^  HTTP/{print $2}')
      [ "$code" == "200" ]
    }
---
# Source: monitoring/charts/grafana/templates/tests/test-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: monitoring-grafana-test
  namespace: monitoring
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
rules:
- apiGroups:      ['policy']
  resources:      ['podsecuritypolicies']
  verbs:          ['use']
  resourceNames:  [monitoring-grafana-test]
---
# Source: monitoring/charts/grafana/templates/tests/test-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: monitoring-grafana-test
  namespace: monitoring
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: monitoring-grafana-test
subjects:
- kind: ServiceAccount
  name: monitoring-grafana-test
  namespace: monitoring
---
# Source: monitoring/charts/grafana/templates/tests/test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: monitoring-grafana-test
  labels:
    helm.sh/chart: grafana-6.43.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/version: "9.2.1"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test-success
    "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"
  namespace: monitoring
spec:
  serviceAccountName: monitoring-grafana-test
  containers:
    - name: monitoring-test
      image: "bats/bats:v1.4.1"
      imagePullPolicy: "IfNotPresent"
      command: ["/opt/bats/bin/bats", "-t", "/tests/run.sh"]
      volumeMounts:
        - mountPath: /tests
          name: tests
          readOnly: true
  volumes:
  - name: tests
    configMap:
      name: monitoring-grafana-test
  restartPolicy: Never
---
# Source: monitoring/charts/mimir-distributed/templates/smoke-test/smoke-test-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: monitoring-mimir-smoke-test
  labels:
    helm.sh/chart: mimir-distributed-4.0.0-weekly.209
    app.kubernetes.io/name: mimir
    app.kubernetes.io/instance: monitoring
    app.kubernetes.io/component: smoke-test
    app.kubernetes.io/version: "r209"
    app.kubernetes.io/managed-by: Helm
  annotations:
    sidecar.istio.io/inject: "false"
    "helm.sh/hook": test
  namespace: "monitoring"
spec:
  backoffLimit: 5
  completions: 1
  parallelism: 1
  selector:
  template:
    metadata:
      labels:
        helm.sh/chart: mimir-distributed-4.0.0-weekly.209
        app.kubernetes.io/name: mimir
        app.kubernetes.io/instance: monitoring
        app.kubernetes.io/version: "r209"
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: smoke-test
    spec:
      serviceAccountName: mimir
      securityContext:
        null
      initContainers:
        []
      containers:
        - name: smoke-test
          image: "grafana/mimir-continuous-test:r209-bc07330"
          imagePullPolicy: 
          args:
            - "-tests.smoke-test"
            - "-tests.write-endpoint=http://monitoring-mimir-nginx.monitoring.svc:80"
            - "-tests.read-endpoint=http://monitoring-mimir-nginx.monitoring.svc:80/prometheus"
            - "-tests.tenant-id="
            - "-tests.write-read-series-test.num-series=1000"
            - "-tests.write-read-series-test.max-query-age=48h"
            - "-server.metrics-port=8080"
          volumeMounts:
          env:
          envFrom:
      restartPolicy: OnFailure
      volumes:
